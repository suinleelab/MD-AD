{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nbbwang/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/job:localhost/replica:0/task:0/device:GPU:0']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras import backend as K\n",
    "from keras.callbacks import CSVLogger, ModelCheckpoint, Callback\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "import datetime\n",
    "import os\n",
    "import numpy as np\n",
    "import h5py\n",
    "import pickle\n",
    "\n",
    "\n",
    "from models import MDAD_model, single_MLP_model, single_linear_model\n",
    "from experiment_helpers import load_final_PCA_data, save_MTL_predictions\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\"\n",
    "K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyperparams = {\"epochs\": [200], \\\n",
    "               \"nonlinearity\": [\"relu\"], \\\n",
    "               \"hidden_sizes_shared\": [[500,100]], \\\n",
    "               \"hidden_sizes_separate\": [[50,10]],\\\n",
    "               \"dropout\":  [.1],\\\n",
    "               \"k_reg\": [.00001,.001],\\\n",
    "               \"learning_rate\": [.0001,.001],\\\n",
    "#                \"learning_rate\": [.001],\\\n",
    "\n",
    "               \"loss_weights\":  [[1, 1]],\\\n",
    "               \"grad_clip_norm\": [.01,.1],\\\n",
    "               \"batch_size\": [20]}\n",
    "hy_dict_list = list(ParameterGrid(hyperparams))\n",
    "len(hy_dict_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200_relu_[500, 100]_[50, 10]_0.100000_0.000010_0.000100_[1, 1]_0.010000_20\n",
      "200_relu_[500, 100]_[50, 10]_0.100000_0.000010_0.001000_[1, 1]_0.010000_20\n",
      "200_relu_[500, 100]_[50, 10]_0.100000_0.001000_0.000100_[1, 1]_0.010000_20\n",
      "200_relu_[500, 100]_[50, 10]_0.100000_0.001000_0.001000_[1, 1]_0.010000_20\n",
      "200_relu_[500, 100]_[50, 10]_0.100000_0.000010_0.000100_[1, 1]_0.100000_20\n",
      "200_relu_[500, 100]_[50, 10]_0.100000_0.000010_0.001000_[1, 1]_0.100000_20\n",
      "200_relu_[500, 100]_[50, 10]_0.100000_0.001000_0.000100_[1, 1]_0.100000_20\n",
      "200_relu_[500, 100]_[50, 10]_0.100000_0.001000_0.001000_[1, 1]_0.100000_20\n"
     ]
    }
   ],
   "source": [
    "for hy_iteration in range(len(hy_dict_list)):\n",
    "\n",
    "#     print(\"HYPERPARAMETER ITERATION: %d\"%hy_iteration)\n",
    "\n",
    "    hy_dict = hy_dict_list[hy_iteration]\n",
    "\n",
    "    num_epochs = hy_dict[\"epochs\"]\n",
    "#     num_epochs = 20\n",
    "    nonlinearity = hy_dict[\"nonlinearity\"]\n",
    "    hidden_sizes_shared = hy_dict[\"hidden_sizes_shared\"]\n",
    "    hidden_sizes_separate = hy_dict[\"hidden_sizes_separate\"]\n",
    "    dropout = hy_dict[\"dropout\"]\n",
    "    k_reg =  hy_dict[\"k_reg\"]\n",
    "    learning_rate = hy_dict[\"learning_rate\"]\n",
    "    loss_weights = hy_dict[\"loss_weights\"]\n",
    "    grad_clip_norm = hy_dict[\"grad_clip_norm\"]\n",
    "    batch_size = hy_dict[\"batch_size\"]\n",
    "\n",
    "#     print(num_epochs, nonlinearity, hidden_sizes_shared, hidden_sizes_separate, dropout, k_reg, learning_rate, \\\n",
    "#           loss_weights, grad_clip_norm, batch_size)\n",
    "    title = \"%d_%s_%s_%s_%f_%f_%f_%s_%f_%d\"%(num_epochs, nonlinearity, str(hidden_sizes_shared), str(hidden_sizes_separate),\\\n",
    "                                       dropout, k_reg, learning_rate, str(loss_weights), grad_clip_norm, batch_size)\n",
    "    print(title)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SPECIFIC_FOLDER = \"origGE\"\n",
    "\n",
    "path_to_all = \"../../md-ad_public_repo_data/Modeling/final_model/%s/\"%SPECIFIC_FOLDER\n",
    "path_to_results = path_to_all + \"results/MLP_baselines/\"\n",
    "path_to_models = path_to_all + \"models/MLP_baselines/\"\n",
    "    \n",
    "    \n",
    "baselines_final_final_model = pickle.load(open(\"../../AD_Project/analyses/MTL_variable_tasks/6vars-continuous/%s/final_models_chosen/MLP_baselines/final.p\"%SPECIFIC_FOLDER, \"rb\" ) )\n",
    "\n",
    "dataset = \"ACT_MSBBRNA_ROSMAP_PCA\"\n",
    "\n",
    "\n",
    "for rep in range(100):\n",
    "  \n",
    "    X,y = load_final_PCA_data(SPECIFIC_FOLDER)\n",
    "\n",
    "    print(X.shape)\n",
    "\n",
    "    for hy_iteration in range(len(hy_dict_list)):\n",
    "\n",
    "        print(\"HYPERPARAMETER ITERATION: %d\"%hy_iteration)\n",
    "\n",
    "        hy_dict = hy_dict_list[hy_iteration]\n",
    "\n",
    "        title = \"%d_%s_%s_%s_%f_%f_%f_%s_%f_%d\"%(hy_dict[\"epochs\"], hy_dict[\"nonlinearity\"], \n",
    "                        str(hy_dict[\"hidden_sizes_shared\"]), str(hy_dict[\"hidden_sizes_separate\"]),\n",
    "                        hy_dict[\"dropout\"], hy_dict[\"k_reg\"], hy_dict[\"learning_rate\"], \n",
    "                        str(hy_dict[\"loss_weights\"]),  hy_dict[\"grad_clip_norm\"], hy_dict[\"batch_size\"])\n",
    "        print(title)\n",
    "            \n",
    "        for phenotype in  [\"ABETA_IHC\", \"TAU_IHC\",\"PLAQUES\", \"TANGLES\",\"BRAAK\", \"CERAD\"]:\n",
    "            print(phenotype)\n",
    "            if title == baselines_final_final_model[phenotype]:\n",
    "\n",
    "                model = single_MLP_model(X, hy_dict) \n",
    "\n",
    "                modelpath = path_to_models + dataset + \"/\" + phenotype + \"/\" +title +  \"/\" + str(rep) + \"/\"\n",
    "                res_dest = path_to_results +  \"/\" + dataset + \"/\" + phenotype +\"/\" + title + \"/\"+ str(rep) + \"/\"\n",
    "                for path in [modelpath, res_dest]:\n",
    "                    if not os.path.isdir(path):\n",
    "                        os.makedirs(path)\n",
    "\n",
    "                csv_logger = CSVLogger(res_dest+'results.log')\n",
    "\n",
    "                History = model.fit(x={'main_input': X},y={'out': y[phenotype]}, \n",
    "                                    verbose=0,epochs=num_epochs, batch_size=batch_size, callbacks=[csv_logger,\\\n",
    "                                    ModelCheckpoint(modelpath+\"{epoch:02d}.hdf5\", monitor='val_loss', verbose=1, \\\n",
    "                                    save_best_only=False, save_weights_only=False, mode='auto', period=200)])\n",
    "\n",
    "                K.clear_session()\n",
    "                gc.collect()   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
