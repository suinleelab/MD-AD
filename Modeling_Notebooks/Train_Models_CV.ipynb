{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nbbwang/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/job:localhost/replica:0/task:0/device:GPU:0']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras import backend as K\n",
    "from keras.callbacks import CSVLogger, ModelCheckpoint, Callback\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "import datetime\n",
    "import os\n",
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "from models import MDAD_model, single_MLP_model, single_linear_model\n",
    "from experiment_helpers import load_data_for_fold, save_MTL_predictions\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\"\n",
    "K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyperparams = {\"epochs\": [200], \\\n",
    "               \"nonlinearity\": [\"relu\"], \\\n",
    "               \"hidden_sizes_shared\": [[500,100]], \\\n",
    "               \"hidden_sizes_separate\": [[50,10]],\\\n",
    "               \"dropout\":  [.1],\\\n",
    "               \"k_reg\": [.00001,.001],\\\n",
    "               \"learning_rate\": [.0001,.001],\\\n",
    "#                \"learning_rate\": [.001],\\\n",
    "               \"loss_weights\":  [[1, 1]],\\\n",
    "               \"grad_clip_norm\": [.01,.1],\\\n",
    "               \"batch_size\": [20]}\n",
    "\n",
    "hy_dict_list = list(ParameterGrid(hyperparams))\n",
    "len(hy_dict_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MD-AD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_folder = \"origGE\"\n",
    "path_to_results = \"../../md-ad_public_repo_data/Modeling/%s/results/MTL/\"%specific_folder\n",
    "path_to_preds = \"../../md-ad_public_repo_data/Modeling/%s/predictions/MTL/\"%specific_folder\n",
    "path_to_models = \"../../md-ad_public_repo_data/Modeling/%s/models/MTL/\"%specific_folder\n",
    "phenotypes = [\"CERAD\", \"PLAQUES\", \"ABETA_IHC\", \"BRAAK\", \"TANGLES\", \"TAU_IHC\"]\n",
    "\n",
    "for data_form in ['ACT_MSBBRNA_ROSMAP_PCASplit']:\n",
    "\n",
    "    print(data_form)\n",
    "\n",
    "    for fold_idx in range(30):\n",
    "        \n",
    "        print(\"FOLD:\",fold_idx)\n",
    "        \n",
    "        X_train, X_valid, y_train, y_valid = load_data_for_fold(fold_idx)\n",
    "        \n",
    "        y_train_dict = {}\n",
    "        y_valid_dict = {}\n",
    "        for p in phenotypes:\n",
    "            y_train_dict[\"%s_out\"%p] = y_train[p]\n",
    "            y_valid_dict[\"%s_out\"%p] = y_valid[p]\n",
    "        \n",
    "\n",
    "        for hy_iteration in range(len(hy_dict_list)):\n",
    "\n",
    "            print(datetime.datetime.now())\n",
    "\n",
    "            print(\"HYPERPARAMETER ITERATION: %d\"%hy_iteration)\n",
    "\n",
    "            hy_dict = hy_dict_list[hy_iteration]\n",
    "\n",
    "            title = \"%d_%s_%s_%s_%f_%f_%f_%s_%f_%d\"%(hy_dict[\"epochs\"], hy_dict[\"nonlinearity\"], \n",
    "                            str(hy_dict[\"hidden_sizes_shared\"]), str(hy_dict[\"hidden_sizes_separate\"]),\n",
    "                            hy_dict[\"dropout\"], hy_dict[\"k_reg\"], hy_dict[\"learning_rate\"], \n",
    "                            str(hy_dict[\"loss_weights\"]),  hy_dict[\"grad_clip_norm\"], hy_dict[\"batch_size\"])\n",
    "            print(title)\n",
    "\n",
    "            res_dest = path_to_results + \"/\" + data_form + \"/\" + title + \"/\"\n",
    "            preds_dest = path_to_preds + \"/\" + data_form + \"/\" + title + \"/\"\n",
    "            modelpath =  path_to_models  + data_form + \"/\" + title + \"/\" + str(fold_idx) + \"/\"\n",
    "\n",
    "            for path in [res_dest, preds_dest, modelpath]:\n",
    "                if not os.path.isdir(path):\n",
    "                    os.makedirs(path)\n",
    "            \n",
    "\n",
    "            model = MDAD_model(X_train, hy_dict)\n",
    "\n",
    "            # https://stackoverflow.com/questions/36895627/python-keras-creating-a-callback-with-one-prediction-for-each-epoch\n",
    "            class prediction_history(Callback):\n",
    "                def __init__(self):\n",
    "                    self.predhis = []\n",
    "                def on_epoch_end(self, epoch, logs={}):\n",
    "                    self.predhis.append(model.predict(X_valid))\n",
    "            predictions=prediction_history()\n",
    "            \n",
    "            \n",
    "            if fold_idx > 24:\n",
    "                History = model.fit(x={'main_input': X_train}, y=y_train_dict, \n",
    "                          validation_data = ({'main_input': X_valid}, y_valid_dict),                                     \n",
    "                          verbose=0,epochs=hy_dict[\"epochs\"], batch_size=hy_dict[\"batch_size\"], \n",
    "                        callbacks=[CSVLogger(res_dest+'%d.log'%fold_idx), predictions, \n",
    "                        # save model:\n",
    "                        ModelCheckpoint(modelpath+\"{epoch:02d}.hdf5\", monitor='val_loss', verbose=0, \\\n",
    "                        save_best_only=False, save_weights_only=False, mode='auto', period=100)])\n",
    "            else:\n",
    "                History = model.fit(x={'main_input': X_train}, y=y_train_dict, \n",
    "                          validation_data = ({'main_input': X_valid}, y_valid_dict),    \n",
    "                          verbose=0, epochs=hy_dict[\"epochs\"], batch_size=hy_dict[\"batch_size\"], \n",
    "                        callbacks=[CSVLogger(res_dest+'%d.log'%fold_idx), predictions])\n",
    "\n",
    "            save_MTL_predictions(predictions, preds_dest, fold_idx, y_valid, phenotypes)\n",
    "            \n",
    "            K.clear_session()\n",
    "            gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP and Linear Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_components = 500\n",
    "phenotypes = [\"CERAD\", \"PLAQUES\", \"ABETA_IHC\", \"BRAAK\", \"TANGLES\", \"TAU_IHC\"]\n",
    "data_form = 'ACT_MSBBRNA_ROSMAP_PCASplit'\n",
    "\n",
    "\n",
    "for baseline_method in [\"Linear_baselines\", \"MLP_baselines\"]:\n",
    "    print(baseline_method)\n",
    "\n",
    "    path_to_results = \"../../md-ad_public_repo_data/Modeling/%s/results/%s/\"%(specific_folder, baseline_method)\n",
    "    path_to_preds = \"../../md-ad_public_repo_data/Modeling/%s/predictions/%s/\"%(specific_folder, baseline_method)\n",
    "    path_to_models = \"../../md-ad_public_repo_data/Modeling/%s/models/%s/\"%(specific_folder, baseline_method)\n",
    "    \n",
    "    for fold_idx in range(30):\n",
    "        \n",
    "        print(\"FOLD:\",fold_idx)\n",
    "        X_train, X_valid, y_train, y_valid = load_data_for_fold(fold_idx)\n",
    "\n",
    "        for hy_iteration in range(len(hy_dict_list)):\n",
    "\n",
    "            print(datetime.datetime.now())\n",
    "\n",
    "            print(\"HYPERPARAMETER ITERATION: %d\"%hy_iteration)\n",
    "\n",
    "            hy_dict = hy_dict_list[hy_iteration]\n",
    "\n",
    "            title = \"%d_%s_%s_%s_%f_%f_%f_%s_%f_%d\"%(hy_dict[\"epochs\"], hy_dict[\"nonlinearity\"], \n",
    "                            str(hy_dict[\"hidden_sizes_shared\"]), str(hy_dict[\"hidden_sizes_separate\"]),\n",
    "                            hy_dict[\"dropout\"], hy_dict[\"k_reg\"], hy_dict[\"learning_rate\"], \n",
    "                            str(hy_dict[\"loss_weights\"]),  hy_dict[\"grad_clip_norm\"], hy_dict[\"batch_size\"])\n",
    "            print(title)\n",
    "\n",
    "            \n",
    "\n",
    "            for phenotype in phenotypes:\n",
    "\n",
    "                print(phenotype)\n",
    "                print(datetime.datetime.now())\n",
    "    \n",
    "                res_dest = path_to_results + \"/\" + data_form + \"/\" + title + \"/\" + phenotype + \"/\"\n",
    "                preds_dest = path_to_preds + \"/\" + data_form + \"/\" + title + \"/\" \n",
    "                modelpath =  path_to_models  + data_form + \"/\" + title + \"/\" + phenotype + \"/\" + str(fold_idx) + \"/\"\n",
    "\n",
    "                for path in [res_dest, preds_dest, modelpath]:\n",
    "                    if not os.path.isdir(path):\n",
    "                        os.makedirs(path)           \n",
    "                  \n",
    "                \n",
    "                if baseline_method == \"MLP_baselines\":\n",
    "                    model = single_MLP_model(X_train, hy_dict)                                        \n",
    "                else:\n",
    "                    model = single_linear_model(X_train, hy_dict)\n",
    "                \n",
    "                print(X_train.shape, X_valid.shape)\n",
    "                print(y_train[phenotype].shape, y_valid[phenotype].shape)\n",
    "\n",
    "\n",
    "                # https://stackoverflow.com/questions/36895627/python-keras-creating-a-callback-with-one-prediction-for-each-epoch\n",
    "                class prediction_history(Callback):\n",
    "                    def __init__(self):\n",
    "                        self.predhis = []\n",
    "                    def on_epoch_end(self, epoch, logs={}):\n",
    "                        self.predhis.append(model.predict(X_valid))\n",
    "\n",
    "                predictions=prediction_history()\n",
    "\n",
    "\n",
    "                csv_logger = CSVLogger(res_dest+ \"/\" + phenotype + \"/\" + '%d.log'%fold_idx)\n",
    "                # And trained it via:\n",
    "                if fold_idx > 24:\n",
    "                    History = model.fit(x={'main_input': X_train}, y=y_train[phenotype], \n",
    "                              validation_data = ({'main_input': X_valid}, y_valid[phenotype]),                                     \n",
    "                              verbose=0,epochs=hy_dict[\"epochs\"], batch_size=hy_dict[\"batch_size\"], \n",
    "                            callbacks=[CSVLogger(res_dest+'%d.log'%fold_idx), predictions, \n",
    "                            # save model:\n",
    "                            ModelCheckpoint(modelpath+\"{epoch:02d}.hdf5\", monitor='val_loss', verbose=0, \\\n",
    "                            save_best_only=False, save_weights_only=False, mode='auto', period=100)])\n",
    "                else:\n",
    "                    History = model.fit(x={'main_input': X_train}, y=y_train[phenotype], \n",
    "                              validation_data = ({'main_input': X_valid}, y_valid[phenotype]),    \n",
    "                              verbose=0, epochs=hy_dict[\"epochs\"], batch_size=hy_dict[\"batch_size\"], \n",
    "                            callbacks=[CSVLogger(res_dest+'%d.log'%fold_idx), predictions])\n",
    "\n",
    "                    \n",
    "                            # SAVE PREDICTIONS\n",
    "                with h5py.File(preds_dest + \"%d.h5\"%fold_idx, 'a') as hf:\n",
    "                    # loop through epochs -- one group is made per epoch\n",
    "                    for i, ep in enumerate(predictions.predhis):\n",
    "                        if \"/%s/%s\"%(str(i),phenotype) in hf:\n",
    "                            del hf[\"/%s/%s\"%(str(i),phenotype)]\n",
    "                        hf.create_dataset(\"/%s/%s\"%(str(i),phenotype), data=predictions.predhis[i], dtype=np.float32)\n",
    "                    if \"/y_true/\"+phenotype in hf:\n",
    "                        del hf[\"/y_true/\"+phenotype]\n",
    "                    hf.create_dataset(\"/y_true/\"+phenotype, data=y_valid[phenotype], dtype=np.float32)\n",
    "\n",
    "\n",
    "                K.clear_session()\n",
    "                gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
