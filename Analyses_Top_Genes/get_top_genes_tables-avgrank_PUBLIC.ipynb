{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nbbwang\\Anaconda2\\envs\\py36\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import h5py\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy.stats import spearmanr, pearsonr, rankdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path_to_average_IGs = \"/Users/nbbwang/Documents/Lee Lab/AD_project/analyses/NEW_FULL_PIPELINE/fgsea/data/IG_weights_forR/origGE/reps/weighted_avg_high_vs_low_path/\"\n",
    "with h5py.File(\"/Users/nbbwang/Documents/Lee Lab/AD_project/analyses/NEW_FULL_PIPELINE/Data_Processing/ProcessedData/combined_files/01_resid_postcombat/ACT_MSBBRNA_ROSMAP.h5\", 'r') as hf:\n",
    "    gene_symbols = hf[\"gene_symbols\"][:].astype(str)\n",
    "\n",
    "path_to_save_scores = \"gene_rankings/MTL/\"\n",
    "\n",
    "    \n",
    "phenotypes = ['CERAD','BRAAK','PLAQUES','TANGLES','ABETA_IHC','TAU_IHC']\n",
    "method = \"MTL\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "phen_dict = {\"all\": ['CERAD','BRAAK','PLAQUES','TANGLES','ABETA_IHC','TAU_IHC'], \n",
    "             \"abeta\": ['CERAD','PLAQUES','ABETA_IHC'],\n",
    "            \"tau\": ['BRAAK','TANGLES','TAU_IHC']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ranks such that most positive value has rank 1\n",
    "def weights_to_rankings(weights, direction=\"positive\"):\n",
    "    if direction==\"positive\":\n",
    "        return  len(weights) - rankdata(weights)\n",
    "    elif direction ==\"negative\":\n",
    "        return len(weights) - rankdata(-1*weights)\n",
    "    else:\n",
    "        return len(weights) - rankdata(abs(weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 14591)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.loadtxt(path_to_average_IGs + \"%s/%i/outputs/G_by_output_type.txt\"%(method,i)).T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n"
     ]
    }
   ],
   "source": [
    "### COMBINE REPS\n",
    "\n",
    "if method==\"MTL\":\n",
    "    num_reps = 101\n",
    "elif method == \"MLP_baselines\":\n",
    "    num_reps = 10\n",
    "    \n",
    "    \n",
    "num_cats_overlaps = num_reps\n",
    "\n",
    "rep_gws = []\n",
    "for i in range(num_reps):\n",
    "    if i%10==0:\n",
    "        print(i)\n",
    "        \n",
    "    p_by_g_gw = np.loadtxt(path_to_average_IGs + \"%s/%i/outputs/G_by_output_type.txt\"%(method,i)).T\n",
    "    rep_gws.append(p_by_g_gw)\n",
    "rep_gws = np.array(rep_gws)\n",
    "\n",
    "rep_ranks = {}\n",
    "for direction in [\"positive\", \"negative\"]:\n",
    "    rep_ranks[direction] = np.zeros([len(rep_gws), len(phenotypes), len(gene_symbols)])\n",
    "    for i in range(num_reps):\n",
    "        for p in range(len(phenotypes)):\n",
    "            rep_ranks[direction][i,p,:] = weights_to_rankings(rep_gws[i,p,:], direction).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### PLOT THE CONSISTENCY OF RANKINGS AS THE NUMBER OF RUNS INCREASES\n",
    "\n",
    "tops = [100,500,1000,2000]\n",
    "agreement_fracs = {}\n",
    "for direction in [\"positive\", \"negative\"]:\n",
    "    print(direction)\n",
    "    agreement_fracs[direction] = []\n",
    "    for top_num in tops:\n",
    "        print(top_num)\n",
    "        agreement_frac = []\n",
    "        currep = 1\n",
    "        cur_top = []\n",
    "\n",
    "        for currep in range(1,101):\n",
    "            new_top = np.argsort(np.mean(np.mean(rep_ranks[direction],axis=1)[:currep,:],axis=0))[:top_num]\n",
    "            agreement_frac.append(len(np.intersect1d(cur_top,new_top))/top_num)\n",
    "\n",
    "            cur_top=new_top\n",
    "        agreement_fracs[direction].append(agreement_frac)\n",
    "\n",
    "cmap=plt.cm.tab20\n",
    "cmaplist = [cmap(i) for i in range(cmap.N)]\n",
    "warm_cmap = matplotlib.colors.LinearSegmentedColormap.from_list(\"\", [cmaplist[3], cmaplist[6]])\n",
    "cold_cmap = matplotlib.colors.LinearSegmentedColormap.from_list(\"\", [cmaplist[5], cmaplist[0]])\n",
    "\n",
    "colors = {\"negative\": [cold_cmap(x) for x in np.arange(4)/3],\n",
    "         \"positive\": [warm_cmap(x) for x in np.arange(4)/3]}\n",
    "\n",
    "f,ax=plt.subplots(2,1,figsize=(8,8))\n",
    "for i,direction in enumerate(agreement_fracs.keys()):\n",
    "    for j,a in enumerate(agreement_fracs[direction]):\n",
    "        ax[i].plot(a[1:], c=colors[direction][j])\n",
    "        ax[i].set_ylim(.6,1.05)\n",
    "\n",
    "        ax[i].legend(tops, bbox_to_anchor=(1.2, 0.5), frameon=False)\n",
    "        ax[i].spines['right'].set_visible(False)\n",
    "        ax[i].spines['top'].set_visible(False)\n",
    "        ax[i].set_xlim(-1,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scores = rep_ranks[\"negative\"]/(len(gene_symbols)-1)\n",
    "consensus_scores = np.mean(scores,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### SAVE rnk FILES (used for gseapy)\n",
    "### SAVE FINAL RANKINGS FOR EACH PHENOTYPE AND COMBINED ACROSS GROUPS\n",
    "\n",
    "\n",
    "if not os.path.isdir(path_to_save_scores):\n",
    "    os.makedirs(path_to_save_scores)\n",
    "\n",
    "\n",
    "for i,p in enumerate(phenotypes):\n",
    "    to_save_c_scores = consensus_scores[i]\n",
    "    \n",
    "    scores_df = pd.DataFrame(np.vstack([gene_symbols,to_save_c_scores]).T, columns=[\"gene\", \"score\"])\n",
    "    scores_df[\"score\"] = scores_df[\"score\"].astype(float)\n",
    "    scores_df = scores_df.sort_values(\"score\", ascending=False)\n",
    "    scores_df = scores_df.reset_index(drop=True)\n",
    "    scores_df.to_csv(\"%s%s.rnk\"%(path_to_save_scores,p), sep=\"\\t\", header=False, index=False)\n",
    "for p_group in [\"all\", \"abeta\", \"tau\"]:\n",
    "    p_idx = np.where(np.in1d(phenotypes, phen_dict[p_group]))[0]\n",
    "    to_save_c_scores = np.mean(consensus_scores[p_idx],axis=0)\n",
    "    scores_df = pd.DataFrame(np.vstack([gene_symbols,to_save_c_scores]).T, columns=[\"gene\", \"score\"])\n",
    "    scores_df[\"score\"] = scores_df[\"score\"].astype(float)\n",
    "    scores_df = scores_df.sort_values(\"score\", ascending=False)\n",
    "    scores_df = scores_df.reset_index(drop=True)\n",
    "    scores_df.to_csv(\"%s%s-related.rnk\"%(path_to_save_scores,p_group), sep=\"\\t\", header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "current_ranking =pd.read_csv(\"%s%s-related.rnk\"%(path_to_save_scores,\"all\"), sep=\"\\t\", names=[\"gene\", \"all-related_score\"])\n",
    "for group in [x+\"-related\" for x in [\"abeta\", \"tau\"]] + phenotypes:\n",
    "    new_df = pd.read_csv(\"%s%s.rnk\"%(path_to_save_scores,group), names=[\"gene\", \"%s_score\"%group], sep=\"\\t\")\n",
    "    current_ranking = current_ranking.merge(new_df, on=\"gene\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "current_ranking.to_csv(path_to_save_scores + \"ALL_CONSENSUS_SCORES.csv\")\n",
    "print(\"Saved rankings to %s\"%path_to_save_scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
