{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PACKAGES\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import itertools\n",
    "import numpy as np\n",
    "import ast\n",
    "import os\n",
    "import h5py\n",
    "import scipy\n",
    "from scipy import stats as ss\n",
    "import pandas as pd\n",
    "import pickle \n",
    "from sklearn import metrics\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from configs import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200_relu_[500, 100]_[50, 10]_0.100000_0.000010_0.001000_[1, 1]_0.100000_20\n",
      "200_relu_[500, 100]_[50, 10]_0.100000_0.001000_0.001000_[1, 1]_0.100000_20\n",
      "200_relu_[500, 100]_[50, 10]_0.100000_0.001000_0.001000_[1, 1]_0.010000_20\n",
      "200_relu_[500, 100]_[50, 10]_0.100000_0.000010_0.001000_[1, 1]_0.100000_20\n",
      "200_relu_[500, 100]_[50, 10]_0.100000_0.001000_0.001000_[1, 1]_0.100000_20\n"
     ]
    }
   ],
   "source": [
    "SPECIFIC_FOLDER = \"origGE\"\n",
    "path_to_results_folder = \"%sresults/\"%CV_save_path\n",
    "path_to_preds_folder = \"%spredictions/\"%CV_save_path\n",
    "\n",
    "path_to_final_chosen_models = \"%sfinal_models_chosen/\"%CV_save_path\n",
    "if  not os.path.isdir(path_to_final_chosen_models):\n",
    "    os.makedirs(path_to_final_chosen_models)\n",
    "    \n",
    "    \n",
    "#### Compute test set variances\n",
    "\n",
    "variances = {}\n",
    "\n",
    "for variable in phenotypes:\n",
    "    vars_by_split = []\n",
    "    for fold_idx in range(25,30):\n",
    "        path_to_preds = path_to_preds_folder + \"MTL\" + \"/\" + split_pca_dataset + \"/\" + '200_relu_[500, 100]_[50, 10]_0.100000_0.000010_0.001000_[1, 1]_0.010000_20' + \"/\" + \"%d.h5\"%fold_idx\n",
    "        with h5py.File(path_to_preds, 'r') as hf:\n",
    "            true_vals = hf[\"y_true\"][variable][:]\n",
    "\n",
    "        vars_by_split.append(np.nanvar(true_vals))    \n",
    "    variances[variable] = np.array(vars_by_split)\n",
    "    \n",
    "\n",
    "def MTL_get_CV_test_res_as_df(path_to_log_files):\n",
    "    \n",
    "    firstfile = os.listdir(path_to_log_files)[0]\n",
    "    cols = pd.read_csv(path_to_log_files + firstfile).columns\n",
    "\n",
    "    test_runs = []\n",
    "    for i in range(5):\n",
    "        # the first 25 files are CV folds\n",
    "        cur_idx = 25+i\n",
    "        test_runs.append(pd.read_csv(path_to_log_files + \"%d.log\"%cur_idx))\n",
    "\n",
    "    test_overall_averages = pd.DataFrame(np.nanmean(np.array([test_runs[i].values for i in range(5)]),axis=0), columns=cols)\n",
    "    return test_runs, test_overall_averages\n",
    "\n",
    "\n",
    "#################################################\n",
    "############### MD-AD  ##########################\n",
    "#################################################\n",
    "\n",
    "performances = {}\n",
    "for var in phenotypes:\n",
    "    performances[var] = {}\n",
    "\n",
    "    for fname in  os.listdir(path_to_results_folder + \"MTL/\" + split_pca_dataset):\n",
    "        test_runs, test_overall_avergaes = MTL_get_CV_test_res_as_df(path_to_results_folder + \"MTL/\" + split_pca_dataset + \"/\" + fname + \"/\")\n",
    "        performances[var][fname] = []\n",
    "\n",
    "        for foldidx in range(5):\n",
    "            # we want the min loss, and we scale by variance\n",
    "            test_var = variances[var][foldidx]\n",
    "            performances[var][fname].append(np.min(test_runs[foldidx][\"val_%s_out_loss\"%var])/test_var)\n",
    "\n",
    "fnames =  os.listdir(path_to_results_folder + \"MTL/\" + split_pca_dataset)\n",
    "num_hy = len(fnames)\n",
    "\n",
    "############  Choose final model within each CV split ##################\n",
    "\n",
    "FOLD_performances = {}\n",
    "for fold_idx in range(5):\n",
    "    FOLD_performances[fold_idx] = {}\n",
    "    for key1 in performances.keys():\n",
    "        FOLD_performances[fold_idx][key1] = []\n",
    "        for key2 in performances[key1].keys():\n",
    "            FOLD_performances[fold_idx][key1].append(performances[key1][key2][fold_idx])\n",
    "            \n",
    "            \n",
    "FOLD_rankings = {}\n",
    "for fold_idx in range(5):\n",
    "\n",
    "    FOLD_rankings[fold_idx] = {}    \n",
    "    for phenotype in FOLD_performances[fold_idx].keys():\n",
    "        if np.sum(~np.isnan(FOLD_performances[fold_idx][phenotype])) == 0:\n",
    "            FOLD_rankings[fold_idx][phenotype] = np.zeros(num_hy)\n",
    "            continue\n",
    "        \n",
    "        FOLD_rankings[fold_idx][phenotype] = ss.rankdata(FOLD_performances[fold_idx][phenotype])\n",
    "\n",
    "        \n",
    "FOLD_sum_of_ranks = {}\n",
    "for fold_idx in range(5):\n",
    "    FOLD_sum_of_ranks[fold_idx] = np.zeros(num_hy)\n",
    "    \n",
    "    for phenotype in FOLD_rankings[fold_idx].keys():\n",
    "        FOLD_sum_of_ranks[fold_idx] += FOLD_rankings[fold_idx][phenotype]\n",
    "\n",
    "final_models_dict = {}\n",
    "for fold_idx in range(5):\n",
    "    print(fnames[np.argmin(FOLD_sum_of_ranks[fold_idx])])\n",
    "    final_models_dict[25+fold_idx] = fnames[np.argmin(FOLD_sum_of_ranks[fold_idx])]\n",
    "    \n",
    "    \n",
    "if not os.path.isdir(path_to_final_chosen_models + \"MTL/\"):\n",
    "    os.makedirs(path_to_final_chosen_models + \"MTL/\")\n",
    "\n",
    "pickle.dump(final_models_dict, open( path_to_final_chosen_models+\"MTL/folds.p\", \"wb\" ) )\n",
    "\n",
    "\n",
    "\n",
    "############  Choose final model overall (for retraining with all data) ##################\n",
    "\n",
    "AVG_performances = {}\n",
    "for key1 in performances.keys():\n",
    "    AVG_performances[key1] = {}\n",
    "    for key2 in performances[key1].keys():\n",
    "        AVG_performances[key1][key2] = np.nanmean(performances[key1][key2])\n",
    "\n",
    "fnames_performances = {}\n",
    "for phenotype in AVG_performances.keys():\n",
    "    fnames_performances[phenotype] = []\n",
    "    for fname in fnames:\n",
    "        fnames_performances[phenotype].append(AVG_performances[phenotype][fname])\n",
    "\n",
    "fnames_rankings = {}\n",
    "for phenotype in fnames_performances.keys():\n",
    "    fnames_rankings[phenotype] = ss.rankdata(fnames_performances[phenotype])\n",
    "    \n",
    "sum_of_ranks = np.zeros(len(fnames))\n",
    "for phenotype in fnames_rankings.keys():\n",
    "    sum_of_ranks += fnames_rankings[phenotype]\n",
    "    \n",
    "pickle.dump(fnames[np.argmin(sum_of_ranks)], open( path_to_final_chosen_models+\"MTL/final.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ABETA_IHC 0.8704339845681205\n",
      "ABETA_IHC 0.8839613403359701\n",
      "ABETA_IHC 0.899973386886023\n",
      "ABETA_IHC 1.0081140715317436\n",
      "ABETA_IHC 0.8650420114604317\n",
      "ABETA_IHC 0.8872312386915923\n",
      "ABETA_IHC 0.8674456162031109\n",
      "ABETA_IHC 0.9191225328080475\n",
      "ABETA_IHC 0.9682548317150054\n",
      "ABETA_IHC 0.8730297887250834\n",
      "ABETA_IHC 0.858636435219681\n",
      "ABETA_IHC 0.7832776889734754\n",
      "ABETA_IHC 0.8045038595147287\n",
      "ABETA_IHC 0.9095874790114935\n",
      "ABETA_IHC 0.763589670657657\n",
      "ABETA_IHC 0.9988770973782479\n",
      "ABETA_IHC 0.8730306498936002\n",
      "ABETA_IHC 0.8793548226333284\n",
      "ABETA_IHC 1.013403479118003\n",
      "ABETA_IHC 0.8535414416924478\n",
      "ABETA_IHC 0.9189623972763498\n",
      "ABETA_IHC 0.8883171809322187\n",
      "ABETA_IHC 0.9088164091637506\n",
      "ABETA_IHC 0.9495541998836919\n",
      "ABETA_IHC 0.8465972237483295\n",
      "ABETA_IHC 0.9262364298687856\n",
      "ABETA_IHC 0.8713289087394259\n",
      "ABETA_IHC 0.8190074557099862\n",
      "ABETA_IHC 1.0145693736223877\n",
      "ABETA_IHC 0.8319564122189561\n",
      "ABETA_IHC 0.8731348339787847\n",
      "ABETA_IHC 0.8621782146685114\n",
      "ABETA_IHC 0.7529871212255717\n",
      "ABETA_IHC 0.8921471889061012\n",
      "ABETA_IHC 0.8267089642855552\n",
      "ABETA_IHC 0.8583952435732861\n",
      "ABETA_IHC 0.832698179200773\n",
      "ABETA_IHC 0.874215792842789\n",
      "ABETA_IHC 0.9649353758777892\n",
      "ABETA_IHC 0.9546880497225808\n",
      "TAU_IHC 0.8720136975341585\n",
      "TAU_IHC 0.609865885304054\n",
      "TAU_IHC 0.7746525432074683\n",
      "TAU_IHC 0.5521260508118843\n",
      "TAU_IHC 0.6456242097601146\n",
      "TAU_IHC 0.8592266724649371\n",
      "TAU_IHC 0.6014852542770023\n",
      "TAU_IHC 0.775605401002609\n",
      "TAU_IHC 0.5569447619816745\n",
      "TAU_IHC 0.6578836049474088\n",
      "TAU_IHC 0.788200361151391\n",
      "TAU_IHC 0.5390073875788718\n",
      "TAU_IHC 0.6382421098861321\n",
      "TAU_IHC 0.5394403727557531\n",
      "TAU_IHC 0.4593356707719574\n",
      "TAU_IHC 0.944917964423644\n",
      "TAU_IHC 0.6877743200481728\n",
      "TAU_IHC 0.8013225804028672\n",
      "TAU_IHC 0.5953084565789046\n",
      "TAU_IHC 0.6499476576718911\n",
      "TAU_IHC 0.9919585012158448\n",
      "TAU_IHC 0.680359956081584\n",
      "TAU_IHC 0.8312475637177997\n",
      "TAU_IHC 0.6390790243120396\n",
      "TAU_IHC 0.6329140102873678\n",
      "TAU_IHC 0.887213191211649\n",
      "TAU_IHC 0.581548210918654\n",
      "TAU_IHC 0.7727096464038831\n",
      "TAU_IHC 0.5420968955202841\n",
      "TAU_IHC 0.5972089049877722\n",
      "TAU_IHC 0.822305487619876\n",
      "TAU_IHC 0.5756944359345252\n",
      "TAU_IHC 0.7021841947245849\n",
      "TAU_IHC 0.5143166163414458\n",
      "TAU_IHC 0.453762284520052\n",
      "TAU_IHC 0.8350878087669604\n",
      "TAU_IHC 0.5694719069534687\n",
      "TAU_IHC 0.7368591784759454\n",
      "TAU_IHC 0.5275782055288625\n",
      "TAU_IHC 0.5579528321773727\n",
      "PLAQUES 0.6680482501057298\n",
      "PLAQUES 0.5965887098926427\n",
      "PLAQUES 0.7768203779464906\n",
      "PLAQUES 0.5539631660173894\n",
      "PLAQUES 0.631259716985767\n",
      "PLAQUES 0.6633673315756481\n",
      "PLAQUES 0.5858749630949182\n",
      "PLAQUES 0.7387993926896965\n",
      "PLAQUES 0.5680160905227114\n",
      "PLAQUES 0.6330793046317067\n",
      "PLAQUES 0.5671306280157516\n",
      "PLAQUES 0.5005827369101716\n",
      "PLAQUES 0.634230404273885\n",
      "PLAQUES 0.4531587677241167\n",
      "PLAQUES 0.5121130009401492\n",
      "PLAQUES 0.6556172051301634\n",
      "PLAQUES 0.6644350853637093\n",
      "PLAQUES 0.6998206494973261\n",
      "PLAQUES 0.579038855698124\n",
      "PLAQUES 0.6654953285658213\n",
      "PLAQUES 0.6378121279180384\n",
      "PLAQUES 0.6343136445016353\n",
      "PLAQUES 0.7116854541611648\n",
      "PLAQUES 0.5605252917764474\n",
      "PLAQUES 0.6387453608845653\n",
      "PLAQUES 0.6848175744644488\n",
      "PLAQUES 0.5967394465739778\n",
      "PLAQUES 0.6405409258789866\n",
      "PLAQUES 0.5953421254835628\n",
      "PLAQUES 0.5876293617221575\n",
      "PLAQUES 0.5766792164884847\n",
      "PLAQUES 0.4771514909253066\n",
      "PLAQUES 0.6356026976553276\n",
      "PLAQUES 0.4676642051261198\n",
      "PLAQUES 0.5003726968589066\n",
      "PLAQUES 0.6344582611418916\n",
      "PLAQUES 0.5936712915736723\n",
      "PLAQUES 0.6827105422796166\n",
      "PLAQUES 0.583559122031991\n",
      "PLAQUES 0.590358244975777\n",
      "TANGLES 0.8950548248022112\n",
      "TANGLES 1.0517496869908538\n",
      "TANGLES 1.0592709848784383\n",
      "TANGLES 0.7703993149687773\n",
      "TANGLES 1.0518423507868022\n",
      "TANGLES 0.92865483087378\n",
      "TANGLES 1.0144224080282256\n",
      "TANGLES 1.0235036973395453\n",
      "TANGLES 0.7504940872576609\n",
      "TANGLES 1.0409559166359765\n",
      "TANGLES 0.8746793969730041\n",
      "TANGLES 1.0153376783815995\n",
      "TANGLES 0.9870857193538425\n",
      "TANGLES 0.7250496116546554\n",
      "TANGLES 0.979653516453264\n",
      "TANGLES 1.1219644657585044\n",
      "TANGLES 1.1711351961275482\n",
      "TANGLES 1.179944886755929\n",
      "TANGLES 0.8806146630370699\n",
      "TANGLES 1.0489012464032905\n",
      "TANGLES 1.067960837109568\n",
      "TANGLES 1.1136778192648586\n",
      "TANGLES 1.1111278886307707\n",
      "TANGLES 0.9114947330484492\n",
      "TANGLES 1.05063136150005\n",
      "TANGLES 0.8906234930111769\n",
      "TANGLES 1.0598584399210158\n",
      "TANGLES 1.078823534878584\n",
      "TANGLES 0.7755143067695605\n",
      "TANGLES 0.980401855729786\n",
      "TANGLES 0.8482000274803467\n",
      "TANGLES 1.004860004056551\n",
      "TANGLES 1.0571799796674461\n",
      "TANGLES 0.7105591541338593\n",
      "TANGLES 1.0291836595716202\n",
      "TANGLES 0.9011716560859816\n",
      "TANGLES 1.0470906198704046\n",
      "TANGLES 1.0481881975999705\n",
      "TANGLES 0.7323744123497371\n",
      "TANGLES 0.9503295067113469\n",
      "BRAAK 0.6293961209072938\n",
      "BRAAK 0.6265139289091103\n",
      "BRAAK 0.7115896660702524\n",
      "BRAAK 0.5488297697153295\n",
      "BRAAK 0.6273895031740093\n",
      "BRAAK 0.6344500328716887\n",
      "BRAAK 0.6644681793367855\n",
      "BRAAK 0.7008561364320847\n",
      "BRAAK 0.5580956086164904\n",
      "BRAAK 0.6198083873946224\n",
      "BRAAK 0.5094384794365053\n",
      "BRAAK 0.5491049720920462\n",
      "BRAAK 0.5853789147608583\n",
      "BRAAK 0.446977132414118\n",
      "BRAAK 0.5431355644744471\n",
      "BRAAK 0.6326477486955817\n",
      "BRAAK 0.6416475549766663\n",
      "BRAAK 0.6802379236469764\n",
      "BRAAK 0.5751179393074577\n",
      "BRAAK 0.6459952657859698\n",
      "BRAAK 0.6161694660517015\n",
      "BRAAK 0.59660441634224\n",
      "BRAAK 0.6850834685081654\n",
      "BRAAK 0.5493591432242136\n",
      "BRAAK 0.5633055110585438\n",
      "BRAAK 0.5793395071034786\n",
      "BRAAK 0.6407403169021628\n",
      "BRAAK 0.6540346608000701\n",
      "BRAAK 0.5225245408218617\n",
      "BRAAK 0.6196388801958951\n",
      "BRAAK 0.5248289821421193\n",
      "BRAAK 0.5288745550172315\n",
      "BRAAK 0.5957031798513974\n",
      "BRAAK 0.43215549846833046\n",
      "BRAAK 0.5244723812283568\n",
      "BRAAK 0.5693716591572259\n",
      "BRAAK 0.6364104177895077\n",
      "BRAAK 0.692793859542574\n",
      "BRAAK 0.5058495363744242\n",
      "BRAAK 0.6089655974179589\n",
      "CERAD 0.6783656248041398\n",
      "CERAD 0.6214610107130943\n",
      "CERAD 0.7208965541180021\n",
      "CERAD 0.567591938792377\n",
      "CERAD 0.6019644497252158\n",
      "CERAD 0.6631660983816627\n",
      "CERAD 0.5955385707847816\n",
      "CERAD 0.7158662617161672\n",
      "CERAD 0.5493163557605193\n",
      "CERAD 0.6080824675442161\n",
      "CERAD 0.6129906221835085\n",
      "CERAD 0.5478518898294459\n",
      "CERAD 0.6096853027522986\n",
      "CERAD 0.5227964969986999\n",
      "CERAD 0.5187012339785974\n",
      "CERAD 0.6163814902236189\n",
      "CERAD 0.5851559349097423\n",
      "CERAD 0.6693040773908951\n",
      "CERAD 0.5609387619192078\n",
      "CERAD 0.5654406549170304\n",
      "CERAD 0.6067169497633481\n",
      "CERAD 0.5937351228318887\n",
      "CERAD 0.6776968997032985\n",
      "CERAD 0.5305824325333431\n",
      "CERAD 0.5638857495244539\n",
      "CERAD 0.6400470582174242\n",
      "CERAD 0.5725237610221817\n",
      "CERAD 0.6645620985110683\n",
      "CERAD 0.5315939488942726\n",
      "CERAD 0.5650828860452441\n",
      "CERAD 0.5923625480137753\n",
      "CERAD 0.5466038541053437\n",
      "CERAD 0.63468575190996\n",
      "CERAD 0.4893698391615726\n",
      "CERAD 0.5492468364054085\n",
      "CERAD 0.6134481995957993\n",
      "CERAD 0.6151715260485062\n",
      "CERAD 0.6579028488850255\n",
      "CERAD 0.5629439742210137\n",
      "CERAD 0.5843476248650008\n",
      "{0: array([33., 36., 10., 40., 32., 34., 12., 19.]), 1: array([35., 28., 11., 41., 37., 30., 10., 24.]), 2: array([40., 37.,  7., 34., 39., 23., 13., 23.]), 3: array([32., 32., 12., 41., 32., 34.,  7., 26.]), 4: array([39., 40., 10., 39., 29., 22., 11., 26.])}\n",
      "FINAL MODELS\n",
      "200_relu_[500, 100]_[50, 10]_0.100000_0.000010_0.001000_[1, 1]_0.010000_20\n",
      "200_relu_[500, 100]_[50, 10]_0.100000_0.000010_0.001000_[1, 1]_0.100000_20\n",
      "200_relu_[500, 100]_[50, 10]_0.100000_0.000010_0.001000_[1, 1]_0.010000_20\n",
      "200_relu_[500, 100]_[50, 10]_0.100000_0.000010_0.001000_[1, 1]_0.100000_20\n",
      "200_relu_[500, 100]_[50, 10]_0.100000_0.000010_0.001000_[1, 1]_0.010000_20\n",
      "FINAL MODEL\n",
      "ABETA_IHC\n",
      "200_relu_[500, 100]_[50, 10]_0.100000_0.000010_0.001000_[1, 1]_0.010000_20\n",
      "TAU_IHC\n",
      "200_relu_[500, 100]_[50, 10]_0.100000_0.000010_0.001000_[1, 1]_0.010000_20\n",
      "PLAQUES\n",
      "200_relu_[500, 100]_[50, 10]_0.100000_0.000010_0.001000_[1, 1]_0.100000_20\n",
      "TANGLES\n",
      "200_relu_[500, 100]_[50, 10]_0.100000_0.000010_0.001000_[1, 1]_0.010000_20\n",
      "BRAAK\n",
      "200_relu_[500, 100]_[50, 10]_0.100000_0.000010_0.001000_[1, 1]_0.100000_20\n",
      "CERAD\n",
      "200_relu_[500, 100]_[50, 10]_0.100000_0.000010_0.001000_[1, 1]_0.010000_20\n"
     ]
    }
   ],
   "source": [
    "#################################################\n",
    "################ MLPs  ##########################\n",
    "#################################################\n",
    "\n",
    "performances = {}\n",
    "for var in phenotypes:\n",
    "\n",
    "    performances[var] = {}\n",
    "\n",
    "    for fname in  os.listdir(path_to_results_folder + \"MLP_baselines/\" + split_pca_dataset):\n",
    "\n",
    "        test_runs, test_overall_avergaes = MTL_get_CV_test_res_as_df(path_to_results_folder + \"MLP_baselines/\" + split_pca_dataset + \"/\" + fname + \"/\" +var + \"/\")\n",
    "\n",
    "        performances[var][fname] = []\n",
    "\n",
    "        for foldidx in range(5):\n",
    "            # we want the min loss, and we scale by variance\n",
    "            test_performance = np.min(test_runs[foldidx][\"val_loss\"])/variances[var][foldidx]\n",
    "            print(var,test_performance)\n",
    "\n",
    "            performances[var][fname].append(test_performance)\n",
    "            \n",
    "############  Choose final model within each CV split ##################\n",
    "            \n",
    "FOLD_performances = {}\n",
    "for fold_idx in range(5):\n",
    "    FOLD_performances[fold_idx] = {}\n",
    "    for key1 in performances.keys():\n",
    "        FOLD_performances[fold_idx][key1] = []\n",
    "        for key2 in performances[key1].keys():\n",
    "            FOLD_performances[fold_idx][key1].append(performances[key1][key2][fold_idx])\n",
    "            \n",
    "            \n",
    "FOLD_rankings = {}\n",
    "for fold_idx in range(5):\n",
    "\n",
    "    FOLD_rankings[fold_idx] = {}\n",
    "    \n",
    "    for phenotype in FOLD_performances[fold_idx].keys():\n",
    "        if np.sum(~np.isnan(FOLD_performances[fold_idx][phenotype])) == 0:\n",
    "            FOLD_rankings[fold_idx][phenotype] = np.zeros(len(fnames))\n",
    "            continue\n",
    "        \n",
    "        FOLD_rankings[fold_idx][phenotype] = ss.rankdata(FOLD_performances[fold_idx][phenotype])\n",
    "\n",
    "final_models = {}\n",
    "for fold_idx in range(5):\n",
    "    final_models[25+fold_idx] = {}\n",
    "    for phenotype in FOLD_rankings[fold_idx].keys():\n",
    "        final_models[25+fold_idx][phenotype] = fnames[np.argmin(FOLD_rankings[fold_idx][phenotype])]\n",
    "\n",
    "if not os.path.isdir(path_to_final_chosen_models + \"MLP_baselines/\"):\n",
    "    os.makedirs(path_to_final_chosen_models + \"MLP_baselines/\")\n",
    "\n",
    "pickle.dump(final_models, open( path_to_final_chosen_models+\"MLP_baselines/folds.p\", \"wb\" ) )\n",
    "    \n",
    "    \n",
    "############  Choose final model overall (for retraining with all data) ##################\n",
    "    \n",
    "    \n",
    "FOLD_sum_of_ranks = {}\n",
    "\n",
    "for fold_idx in range(5):\n",
    "    FOLD_sum_of_ranks[fold_idx] = np.zeros(len(fnames))\n",
    "    \n",
    "    for phenotype in FOLD_rankings[fold_idx].keys():\n",
    "        FOLD_sum_of_ranks[fold_idx] += FOLD_rankings[fold_idx][phenotype]\n",
    "\n",
    "print(FOLD_sum_of_ranks)\n",
    "print(\"FINAL MODELS\")\n",
    "for fold_idx in range(5):\n",
    "    print(fnames[np.argmin(FOLD_sum_of_ranks[fold_idx])])\n",
    "    \n",
    "    \n",
    "AVG_performances = {}\n",
    "for key1 in performances.keys():\n",
    "    AVG_performances[key1] = {}\n",
    "    for key2 in performances[key1].keys():\n",
    "        AVG_performances[key1][key2] = np.nanmean(performances[key1][key2])\n",
    "\n",
    "fnames_performances = {}\n",
    "for phenotype in AVG_performances.keys():\n",
    "    fnames_performances[phenotype] = []\n",
    "    for fname in fnames:\n",
    "        fnames_performances[phenotype].append(AVG_performances[phenotype][fname])\n",
    "        \n",
    "fnames_rankings = {}\n",
    "for phenotype in fnames_performances.keys():\n",
    "    fnames_rankings[phenotype] = ss.rankdata(fnames_performances[phenotype])\n",
    "    \n",
    "print(\"FINAL MODEL\")\n",
    "\n",
    "final_final_mlp_baselines = {}\n",
    "for phenotype in fnames_rankings.keys():\n",
    "    print(phenotype)\n",
    "    print(fnames[np.argmin(fnames_rankings[phenotype])])\n",
    "    final_final_mlp_baselines[phenotype]= fnames[np.argmin(fnames_rankings[phenotype])]\n",
    "    \n",
    "    \n",
    "\n",
    "pickle.dump(final_final_mlp_baselines, open( path_to_final_chosen_models+\"MLP_baselines/final.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
