{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nbbwang/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from keras.layers import Input, Dense, Dropout\n",
    "from keras import optimizers, regularizers, losses\n",
    "\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras.callbacks import CSVLogger\n",
    "from keras import metrics\n",
    "\n",
    "import pickle\n",
    "import h5py\n",
    "import keras\n",
    "\n",
    "import sys\n",
    "\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "K.tensorflow_backend._get_available_gpus()\n",
    "\n",
    "\n",
    "\n",
    "from configs import * \n",
    "from models import ignorenans_categorical_accuracy, ordloss, ignorenans_mse, ignorenans_scaled_mse\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# mode = \"TEST\"\n",
    "mode = \"TRAIN\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Point to model files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "path_to_models = \"../../AD_Project/analyses/MTL_variable_tasks/6vars-continuous/%s/models/\"%SPECIFIC_FOLDER\n",
    "path_to_models = \"../../Pipeline_Outputs_Submitted/%s/models/\"%SPECIFIC_FOLDER\n",
    "\n",
    "path_to_split_data = path_to_MDAD_data_folders + \"%s/%s\"%(SPECIFIC_FOLDER, split_pca_dataset)\n",
    "\n",
    "MTL_FINAL_MODELS = pickle.load(open(path_to_final_models_chosen + \"MTL/folds.p\", \"rb\" ) )\n",
    "MLP_BASELINES_FINAL_MODELS =pickle.load(open(path_to_final_models_chosen + \"MLP_baselines/folds.p\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_model_layers(model_file, num_layers):\n",
    "    \n",
    "    # note: need to define custom functions for model in order to load, but these don't actually get used\n",
    "    model = keras.models.load_model(model_file, custom_objects={\"ordloss_cur_params\": ordloss(0), \\\n",
    "            \"ignorenans_mse\": ignorenans_mse, \"cat_acc\": ignorenans_categorical_accuracy(0), \\\n",
    "            \"ignorenans_scaled_mse\": ignorenans_scaled_mse})\n",
    "    \n",
    "    # define new model that cuts off the last several layers\n",
    "    newmodel = Model(inputs = model.input, outputs = model.layers[num_layers-1].output)\n",
    "    \n",
    "    # agian, need to specify these parameters, but they aren't used since we don't retrain the model\n",
    "    opt = optimizers.adam()  \n",
    "    newmodel.compile(optimizer=opt, loss= \"mse\")\n",
    "    \n",
    "    return newmodel\n",
    "\n",
    "\n",
    "# get centroids for a new set of points\n",
    "def kmeans_centroids_for_test(X_test, cluster_labels):\n",
    "    n_clusters = len(np.unique(cluster_labels))\n",
    "    n,d = X_test.shape\n",
    "    \n",
    "    new_centroids = np.zeros([n_clusters,n])\n",
    "    for i in range(n_clusters):\n",
    "        new_centroids[i] = np.mean(X_test.T[np.where(cluster_labels ==i)], axis=0).reshape([1,-1])\n",
    "\n",
    "    return new_centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get model slice (up to last shared layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# last shared layer number \n",
    "layer_number = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform data with models (MTL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nbbwang/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:96: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last_shared_layer_transformations_TRAIN/origGE/MTL/200_relu_[500, 100]_[50, 10]_0.100000_0.000010_0.001000_[1, 1]_0.100000_20/25.h5\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking : expected main_input to have shape (500,) but got array with shape (100,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-0371f7337faa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mhf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"labels\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0mhf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"outputs\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0mhf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"labels_names\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1780\u001b[0m         x = _standardize_input_data(x, self._feed_input_names,\n\u001b[1;32m   1781\u001b[0m                                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1782\u001b[0;31m                                     check_batch_axis=False)\n\u001b[0m\u001b[1;32m   1783\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1784\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    118\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    121\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking : expected main_input to have shape (500,) but got array with shape (100,)"
     ]
    }
   ],
   "source": [
    "if mode == \"TEST\":\n",
    "    path_to_new_files = \"last_shared_layer_transformations/%s/MTL/\"%SPECIFIC_FOLDER\n",
    "else:\n",
    "    path_to_new_files = \"last_shared_layer_transformations_TRAIN/%s/MTL/\"%SPECIFIC_FOLDER    \n",
    "    \n",
    "for fold_idx in range(25,30):\n",
    "\n",
    "    hy_name = MTL_FINAL_MODELS[fold_idx]\n",
    "    \n",
    "    with h5py.File(path_to_split_data + \"/\" + str(fold_idx) + \".h5\", 'r') as hf:\n",
    "        X_train = hf[\"X_train_transformed\"][:,:num_components].astype(np.float64)\n",
    "        X_valid = hf[\"X_valid_transformed\"][:,:num_components].astype(np.float64)\n",
    "        labels_train = hf[\"y_train\"][:]\n",
    "        labels_valid = hf[\"y_valid\"][:]\n",
    "        labels_names = hf[\"labels_names\"][:]\n",
    "\n",
    "\n",
    "    model = get_model_layers(path_to_models + \"MTL/ACT_MSBBRNA_ROSMAP_PCASplit/%s/%i/%i.hdf5\"%(hy_name, fold_idx, 200), 4)\n",
    "\n",
    "    print(path_to_new_files+ \"%s/%i.h5\"%(hy_name, fold_idx))\n",
    "    if not os.path.isdir(path_to_new_files+ hy_name + \"/\"):\n",
    "        os.makedirs(path_to_new_files+ hy_name + \"/\")\n",
    "\n",
    "    with h5py.File(path_to_new_files + \"%s/%i.h5\"%(hy_name, fold_idx), 'w') as hf:\n",
    "        if mode==\"TEST\":\n",
    "            hf.create_dataset(\"labels\", data=labels_valid)\n",
    "            hf.create_dataset(\"outputs\", data=model.predict(X_valid))\n",
    "        else:\n",
    "            hf.create_dataset(\"labels\", data=labels_train)\n",
    "            hf.create_dataset(\"outputs\", data=model.predict(X_train))\n",
    "        hf.create_dataset(\"labels_names\", data=labels_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform data with models (MLP baselines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if mode == \"TEST\":\n",
    "    path_to_new_files = \"last_shared_layer_transformations/%s/MLP_baselines/\"%SPECIFIC_FOLDER\n",
    "else:\n",
    "    path_to_new_files = \"last_shared_layer_transformations_TRAIN/%s/MLP_baselines/\"%SPECIFIC_FOLDER\n",
    "    \n",
    "\n",
    "\n",
    "for fold_idx in range(25,30):\n",
    "\n",
    "    with h5py.File(path_to_split_data + \"/\" + str(fold_idx) + \".h5\", 'r') as hf:\n",
    "        X_train = hf[\"X_train_transformed\"][:,:num_components].astype(np.float64)\n",
    "        X_valid = hf[\"X_valid_transformed\"][:,:num_components].astype(np.float64)\n",
    "        labels_train = hf[\"y_train\"][:]\n",
    "        labels_valid = hf[\"y_valid\"][:]\n",
    "        labels_names = hf[\"labels_names\"][:]\n",
    "\n",
    "\n",
    "    for phenotype in [\"ABETA_IHC\", \"TAU_IHC\", \"CERAD\", \"BRAAK\", \"PLAQUES\", \"TANGLES\"]:  \n",
    "        \n",
    "        MLP_final_path = path_to_models + \"MLP_baselines/\" + split_pca_dataset + \"/\"\n",
    "        \n",
    "        \n",
    "        \n",
    "        hy_name = MLP_BASELINES_FINAL_MODELS[fold_idx][phenotype]\n",
    "\n",
    "        model = get_model_layers(MLP_final_path + \"%s/%s/%i/%i.hdf5\"%(hy_name, phenotype, fold_idx, 200), 4)\n",
    "\n",
    "        print(path_to_new_files+ \"%s/%s/%i.h5\"%(hy_name, phenotype, fold_idx))\n",
    "        if not os.path.isdir(path_to_new_files+ hy_name + \"/\" + phenotype):\n",
    "            os.makedirs(path_to_new_files+ hy_name + \"/\" + phenotype)\n",
    "\n",
    "        with h5py.File(path_to_new_files + \"%s/%s/%i.h5\"%(hy_name, phenotype, fold_idx), 'w') as hf:\n",
    "            if mode==\"TEST\":\n",
    "                hf.create_dataset(\"labels\", data=labels_valid)\n",
    "                hf.create_dataset(\"outputs\", data=model.predict(X_valid))\n",
    "            else:\n",
    "                hf.create_dataset(\"labels\", data=labels_train)\n",
    "                hf.create_dataset(\"outputs\", data=model.predict(X_train))\n",
    "            hf.create_dataset(\"labels_names\", data=labels_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if mode ==\"TRAIN\":\n",
    "    path_to_new_files = \"last_shared_layer_transformations_TRAIN/%s/unsupervised_methods/\"%SPECIFIC_FOLDER\n",
    "else:\n",
    "    path_to_new_files = \"last_shared_layer_transformations/%s/unsupervised_methods/\"%SPECIFIC_FOLDER\n",
    "    \n",
    "for fold_idx in range(25,30):\n",
    "    \n",
    "    with h5py.File(path_to_split_data + \"/%i.h5\"%fold_idx, 'r') as hf:\n",
    "        if mode == \"TRAIN\":\n",
    "            X = hf[\"X_train_transformed\"][:,:num_components].astype(np.float64)\n",
    "            labels = hf[\"y_train\"][:]\n",
    "\n",
    "        else:\n",
    "            X = hf[\"X_valid_transformed\"][:,:num_components].astype(np.float64)\n",
    "            labels = hf[\"y_valid\"][:]\n",
    "            \n",
    "        X_train =  hf[\"X_train_transformed\"][:,:num_components].astype(np.float64)\n",
    "        gene_symbols = hf[\"gene_symbols\"][:]\n",
    "        labels_names = hf[\"labels_names\"][:]\n",
    "\n",
    "        \n",
    "    ####### PCA STUFF\n",
    "    \n",
    "    for transformation in [\"KMeans\", \"PCA\"]:\n",
    "        \n",
    "        num_dims = 100\n",
    "\n",
    "        print(path_to_new_files+ \"%s/%i.h5\"%(transformation, fold_idx))\n",
    "        if not os.path.isdir(path_to_new_files + transformation + \"/\"):\n",
    "            os.makedirs(path_to_new_files + transformation + \"/\")\n",
    "\n",
    "        if transformation == \"KMeans\":\n",
    "            kmeans = KMeans(n_clusters=num_dims).fit(X_train.T)\n",
    "            X_transformed = kmeans_centroids_for_test(X, kmeans.labels_).T\n",
    "    \n",
    "        elif transformation == \"PCA\":\n",
    "            pca = PCA(n_components =num_dims)\n",
    "            pca.fit(X_train)\n",
    "            X_transformed = pca.transform(X)[:, :num_components]\n",
    "\n",
    "\n",
    "        print(labels.shape, X_transformed.shape)\n",
    "        with h5py.File(path_to_new_files + \"%s/%i.h5\"%(transformation, fold_idx), 'w') as hf:\n",
    "            hf.create_dataset(\"labels\", data=labels)\n",
    "            hf.create_dataset(\"outputs\", data=X_transformed)\n",
    "            hf.create_dataset(\"labels_names\", data=labels_names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
